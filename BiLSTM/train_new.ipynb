{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 16:55:29.832329: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-14 16:55:29.924113: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-14 16:55:29.924127: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-14 16:55:29.942580: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-14 16:55:30.374163: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-14 16:55:30.374217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-14 16:55:30.374222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# https://data.bls.gov/pdq/SurveyOutputServlet\n",
    "\n",
    "# importing all the necessary libraries!\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from keras.regularizers import L1L2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import mlflow\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create all the necessary directory!\n",
    "\n",
    "def create_dir(path):\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path, exist_ok = False)\n",
    "            print(\"New directory is created\")\n",
    "\n",
    "\n",
    "# dumping all the hyperparameters to json file!\n",
    "def hyperparms(dictionary):\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(dictionary, indent=4)\n",
    "    \n",
    "    # Writing to sample.json\n",
    "    with open(path_metrics +'/'+ 'hyperparm.json', \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "#load and format data\n",
    "\n",
    "class DataFormatting():\n",
    "      \n",
    "    def __init__(self):\n",
    "       \n",
    "        self.df_data = None\n",
    "        self.df_datetime = None\n",
    "\n",
    "    def dataset(df):\n",
    "\n",
    "        # converting time colum from object type to datetime format\n",
    "        df['date'] = pd.to_datetime(df['date'],dayfirst = True, format = '%Y-%m-%d')\n",
    "        # creating a ema feature\n",
    "        #df['SMA_10'] = df[['close']].rolling(10).mean().shift(1)\n",
    "        #df['SMA_50'] = df[['close']].rolling(50).mean().shift(1)\n",
    "        #df['SMA_200'] = df[['close']].rolling(200).mean().shift(1)\n",
    "        df = df.dropna()\n",
    "        # splitting the dataframe in to X and y \n",
    "        df_data = df[['open','high','low','close','CPI','INTEREST_RATE']] #'high','low',,'CRUDE_OIL_CLOSE','US500_CLOSE','open','EXCHANGE_RATE',\n",
    "        df_datetime =df[['date']]\n",
    "\n",
    "        return df_data, df_datetime\n",
    "\n",
    "\n",
    "# Data transformation (changing data shape to model requirement)\n",
    "\n",
    "def data_transformation(data, lags = 5, n_fut = 1):\n",
    "    \n",
    "    \"\"\" this function transforms dataframe to required input shape for the model.\n",
    "    It required 2 input arguments:\n",
    "    1. data: this will be the pandas dataframe\n",
    "    2. lags: how many previous price points to be used to predict the next future value, in\n",
    "    this case the default is set to 5 for 'XAUUSD' commodity\"\"\"\n",
    "\n",
    "    # initialize lists to store the dataset\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(lags, len(data)- n_fut +1):\n",
    "        X_data.append(data[i-lags: i, 0: data.shape[1]])\n",
    "        y_data.append(data[i+ n_fut-1:i+n_fut,3]) # extracts close price with specific lag as price to be predicted.\n",
    "\n",
    "    # convert the list to numpy array\n",
    "\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return X_data, y_data\n",
    "\n",
    "class LSTM_model():\n",
    "    \n",
    "\n",
    "    def __init__(self,n_hidden_layers, units, dropout, train_data_X, train_data_y, epochs, reg):\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.train_data_X = train_data_X\n",
    "        self.train_data_y = train_data_y\n",
    "        self.epochs = epochs\n",
    "        self.reg = reg\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        # first lstm layer\n",
    "        model.add(LSTM(self.units, activation='tanh', input_shape=(self.train_data_X.shape[1], self.train_data_X.shape[2]), kernel_regularizer=self.reg, return_sequences=True))\n",
    "\n",
    "        if self.n_hidden_layers !=1:\n",
    "\n",
    "            # building hidden layers\n",
    "            for i in range(1, self.n_hidden_layers):\n",
    "                # for the last layer as the return sequence is False\n",
    "                if i == self.n_hidden_layers -1:\n",
    "                    model.add(LSTM(int(self.units/(2**i)),  activation='tanh', return_sequences=False))\n",
    "                else:\n",
    "                    model.add(LSTM(int(self.units/(2**i)),  activation='tanh', return_sequences=True))\n",
    "\n",
    "        else:\n",
    "            model.add(LSTM(int(self.units/2),  activation='tanh', return_sequences=False))\n",
    "\n",
    "        # adding dropout layer\n",
    "        model.add(Dropout(self.dropout))\n",
    "        # final layer\n",
    "        model.add(Dense(self.train_data_y.shape[1]))\n",
    "\n",
    "        return model\n",
    "\n",
    "# def build_model(space): #train_data_X.shape[1], train_data_X.shape[2]\n",
    "\n",
    "#     with mlflow.start_run():\n",
    "#         mlflow.set_tag('model','lstm')\n",
    "#         mlflow.log_params(space)\n",
    "        \n",
    "#         model = Sequential()\n",
    "#             # first lstm layer\n",
    "#         model.add(LSTM(units = space['units'], activation='tanh', input_shape=(train_data_X.shape[1],train_data_X.shape[2]), kernel_regularizer=L1L2(l1=space['l1'], l2=space['l2']), return_sequences=True))\n",
    "\n",
    "#         if space['layers'] !=1:\n",
    "\n",
    "#             # building hidden layers\n",
    "#             for i in range(1, space['layers']):\n",
    "#                 # for the last layer as the return sequence is False\n",
    "#                 if i == space['layers'] -1:\n",
    "#                     model.add(LSTM(int(space['units']/(2**i)),  activation='tanh', return_sequences=False))\n",
    "#                 else:\n",
    "#                     model.add(LSTM(int(space['units']/(2**i)),  activation='tanh', return_sequences=True))\n",
    "\n",
    "#         else:\n",
    "#             model.add(LSTM(int(space['units']/2),  activation='tanh', return_sequences=False))\n",
    "\n",
    "#         # adding dropout layer\n",
    "#         model.add(Dropout(space['dropout']))\n",
    "#         # final layer\n",
    "#         model.add(Dense(train_data_y.shape[1])) #train_data_y.shape[1]\n",
    "#         # metrics for evaluating the model\n",
    "#         metrics = [tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanAbsolutePercentageError()]\n",
    "\n",
    "#         # model compiler\n",
    "#         model.compile(optimizer=Adam(learning_rate = space['rate']), loss='mse', metrics = metrics)\n",
    "\n",
    "#         # setting the callback function\n",
    "#         cb = [\n",
    "#             tf.keras.callbacks.ModelCheckpoint(path_checkpoint),\n",
    "#             tf.keras.callbacks.CSVLogger(path_metrics+'/'+'data.csv')]\n",
    "\n",
    "#         # model fitting protocol\n",
    "#         history = model.fit(train_data_X,train_data_y, \n",
    "#                             epochs = 500, \n",
    "#                             batch_size = space['batch_size'],  \n",
    "#                             validation_split=0.1,\n",
    "#                             verbose = 1,\n",
    "#                             callbacks=[cb],\n",
    "#                             shuffle= False,\n",
    "#                             workers = 8,\n",
    "#                             use_multiprocessing = True)\n",
    "        \n",
    "#         # training dataset\n",
    "#         train_loss, RMSE, MAE, MAPE = model.evaluate(train_data_X,train_data_y)\n",
    "#         print('\\n','Evaluation of Training dataset:','\\n''\\n','train_loss:',round(train_loss,3),'\\n','RMSE:',round(RMSE,3),'\\n', 'MAE:',round(MAE,3),'\\n','MAPE:',round(MAPE,3))\n",
    "#         mlflow.log_metric('train_loss',train_loss)\n",
    "#         mlflow.log_metric('RMSE', RMSE)\n",
    "#         mlflow.log_metric('MAE', MAE)\n",
    "#         mlflow.log_metric('MAPE',MAPE)\n",
    "\n",
    "    \n",
    "#         return {'loss': MAPE, 'status': STATUS_OK, 'model': model, 'space':space}\n",
    "\n",
    "\n",
    "class Bi_LSTM_model():\n",
    "    \n",
    "\n",
    "    def __init__(self,n_hidden_layers, units, dropout, train_data_X, train_data_y, epochs, reg):\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.train_data_X = train_data_X\n",
    "        self.train_data_y = train_data_y\n",
    "        self.epochs = epochs\n",
    "        self.reg = reg\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        # first lstm layer\n",
    "        model.add(Bidirectional(LSTM(self.units, activation='tanh', input_shape=(self.train_data_X.shape[1], self.train_data_X.shape[2]), kernel_regularizer=self.reg, return_sequences=True)))\n",
    "        # building hidden layers\n",
    "        \n",
    "        if self.n_hidden_layers !=1:\n",
    "\n",
    "            for i in range(1, self.n_hidden_layers):\n",
    "                # for the last layer as the return sequence is False\n",
    "                if i == self.n_hidden_layers -1:\n",
    "                    model.add(Bidirectional(LSTM(int(self.units/(2**i)),  activation='tanh', return_sequences=False)))\n",
    "                else:\n",
    "                    model.add(Bidirectional(LSTM(int(self.units/(2**i)),  activation='tanh', return_sequences=True)))\n",
    "\n",
    "        else:\n",
    "            model.add(Bidirectional(LSTM(int(self.units/2),  activation='tanh', return_sequences=False)))\n",
    "        \n",
    "        # adding dropout layer\n",
    "        model.add(Dropout(self.dropout))\n",
    "        # final layer\n",
    "        model.add(Dense(self.train_data_y.shape[1]))\n",
    "    \n",
    "        return model\n",
    "\n",
    "def metricplot(df, xlab, ylab_1,ylab_2, path):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function plots metric curves and saves it\n",
    "    to respective folder\n",
    "    inputs: df : pandas dataframe \n",
    "            xlab: x-axis\n",
    "            ylab_1 : yaxis_1\n",
    "            ylab_2 : yaxis_2\n",
    "            path: full path for saving the plot\n",
    "            \"\"\"\n",
    "    plt.figure()\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    sns.lineplot(x = df[xlab], y = df[ylab_1])\n",
    "    sns.lineplot(x = df[xlab], y = df[ylab_2])\n",
    "    plt.xlabel('Epochs',fontsize = 12)\n",
    "    plt.ylabel(ylab_1,fontsize = 12)\n",
    "    plt.xticks(fontsize = 12)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    plt.legend([ylab_1,ylab_2], prop={\"size\":12})\n",
    "    plt.savefig(path+'/'+ ylab_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedModelRegistryStoreURIException",
     "evalue": " Model registry functionality is unavailable; got unsupported URI 'gs://mlflow-bucket-jay/artifacts --host 10.132.0.2:5000' for model registry data storage. Supported URI schemes are: ['', 'file', 'databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql']. See https://www.mlflow.org/docs/latest/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/registry.py\u001b[0m in \u001b[0;36mget_store_builder\u001b[0;34m(self, store_uri)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mstore_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnsupportedModelRegistryStoreURIException\u001b[0m Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59263/2721130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#mlflow.set_tracking_uri(\"sqlite:///lstm.db\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tracking_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://mlflow-bucket-jay/artifacts --host 10.132.0.2:5000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gold_Price_Forecasting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautolog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexperiment_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_experiment_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tracking_uri, registry_uri)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mfinal_tracking_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_tracking_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_registry_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregistry_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracking_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrackingServiceClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_tracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;31m# `MlflowClient` also references a `ModelRegistryClient` instance that is provided by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# `MlflowClient._get_registry_client()` method. This `ModelRegistryClient` is not explicitly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tracking_uri)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# property method to ensure that the client is serializable, even if the store is not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# self.store  # pylint: disable=pointless-statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/utils.py\u001b[0m in \u001b[0;36m_get_store\u001b[0;34m(store_uri, artifact_uri)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tracking_store_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/registry.py\u001b[0m in \u001b[0;36mget_store\u001b[0;34m(self, store_uri, artifact_uri)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mresolved_store_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_tracking_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_store_with_resolved_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_store_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/registry.py\u001b[0m in \u001b[0;36m_get_store_with_resolved_uri\u001b[0;34m(self, resolved_store_uri, artifact_uri)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0mexternal\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_store_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_store_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolved_store_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mlflow/tracking/registry.py\u001b[0m in \u001b[0;36mget_store_builder\u001b[0;34m(self, store_uri)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mstore_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise UnsupportedModelRegistryStoreURIException(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0munsupported_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupported_uri_schemes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             )\n",
      "\u001b[0;31mUnsupportedModelRegistryStoreURIException\u001b[0m:  Model registry functionality is unavailable; got unsupported URI 'gs://mlflow-bucket-jay/artifacts --host 10.132.0.2:5000' for model registry data storage. Supported URI schemes are: ['', 'file', 'databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql']. See https://www.mlflow.org/docs/latest/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations."
     ]
    }
   ],
   "source": [
    "#DATEBASE_NAME = input('Enter new database name:')\n",
    "#mlflow.set_tracking_uri(\"sqlite:///lstm.db\")\n",
    "#mlflow.set_tracking_uri(\"gs://mlflow-bucket-jay/artifacts --host 10.132.0.2:5000\")\n",
    "mlflow.set_experiment(\"Gold_Price_Forecasting\")\n",
    "mlflow.keras.autolog()\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag('mleng','Jayesh')\n",
    "    # loading the dataset!\n",
    "    data = pd.read_csv('../data/macro_data/Gold_Macor_Data.csv',index_col=[0]) \n",
    "\n",
    "    #mlflow.log_param('train_data','../data/macro_data/Gold_Macor_Data.csv')\n",
    "    # dropping rows iteratively from bottom for forecasting\n",
    "    #data.drop(index=data.index[-j:],axis=0, inplace=True) \n",
    "    seed(42)\n",
    "    tf.random.set_seed(42) \n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    # hyperparameters\n",
    "    train_size_percent = 90\n",
    "    lag = 2\n",
    "    n_fut = 1\n",
    "    n_hidden_layers = 3\n",
    "    batch_size = 128 #256\n",
    "    units = 192 \n",
    "    dropout = 0\n",
    "    epochs = 700\n",
    "    learning_rate = 1.22556185e-05\n",
    "    l1 = 0.001758323799832654\n",
    "    l2 = 0.04727729600605428\n",
    "    reg = L1L2(l1=l1, l2=l2)\n",
    "\n",
    "    # creating main folder\n",
    "    today = datetime.now()\n",
    "    today  = today.strftime('%Y_%m_%d')\n",
    "    path = '../Model_Outputs/'+ today\n",
    "    create_dir(path)\n",
    "\n",
    "    # Which model to run: \n",
    "# MODEL_NAME = input('Enter the name LSTM or BILSTM:')\n",
    "    # creating directory to save model and its output\n",
    "    EXPERIMENT_NAME = input('Enter new Experiment name:')\n",
    "    print('\\n')\n",
    "    print('A folder with',EXPERIMENT_NAME,'name has be created to store all the model details!')\n",
    "    print('\\n')\n",
    "    folder = EXPERIMENT_NAME\n",
    "    path_main = path + '/'+ folder\n",
    "    create_dir(path_main)\n",
    "\n",
    "    # creating directory to save model and its output\n",
    "    folder = 'model_Bilstm'#+ str(units) + '_' + str(n_hidden_layers)\n",
    "    path_dir = path_main + '/'+ folder\n",
    "    create_dir(path_dir)\n",
    "\n",
    "    # creating directory to save all the metric data\n",
    "    folder = 'metrics'\n",
    "    path_metrics = path_dir +'/'+ folder\n",
    "    create_dir(path_metrics)\n",
    "\n",
    "    # creating folder to save model.h5 file\n",
    "    folder = 'model'\n",
    "    path_model = path_dir +'/'+ folder\n",
    "    create_dir(path_model)\n",
    "\n",
    "    # creating folder to save model.h5 file\n",
    "    folder = 'model_checkpoint'\n",
    "    path_checkpoint = path_dir +'/'+ folder\n",
    "    create_dir(path_checkpoint)\n",
    "\n",
    "    # creating folder to save model.h5 file\n",
    "    folder = 'forecasting_resutls'\n",
    "    path_forecast = path_dir +'/'+ folder\n",
    "    create_dir(path_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initializing DataFormatting class\n",
    "data_init = DataFormatting()\n",
    "df_data, df_datetime = DataFormatting.dataset(data)\n",
    "print('\\n')\n",
    "print('Displaying top 5 rows of the dataset:')\n",
    "print('\\n')\n",
    "print(df_data.head())\n",
    "print(df_data.shape)\n",
    "print(df_data.columns)\n",
    "df_colnames = list(df_data.columns)\n",
    "# normalize train, val and test dataset\n",
    "\n",
    "# initialize StandartScaler()\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_data)\n",
    "data_fit_transformed = scaler.transform(df_data)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Displaying top 5 rows of all the scaled dataset:')\n",
    "print('\\n')\n",
    "#print('The train dateset:','\\n''\\n',data_fit_transformed[0:5],'\\n''\\n', 'The validation dataset:','\\n''\\n',val_transformed[0:5],'\\n''\\n','The test dataset:','\\n''\\n',test_transformed[0:5])\n",
    "print('The train dateset:','\\n''\\n',data_fit_transformed[0:10])\n",
    "\n",
    "# changing shape of the data to match the model requirement!\n",
    "\n",
    "X_data, y_data = data_transformation(data_fit_transformed, lags = lag, n_fut = n_fut)\n",
    "print('\\n')\n",
    "print('Displaying the shape of the dataset required by the model:')\n",
    "print('\\n')\n",
    "print(f' Input shape X:',X_data.shape, f'Input shape y:',y_data.shape)\n",
    "print('\\n')\n",
    "#print(X_data)\n",
    "print(y_data[0:10])\n",
    "#setting the model file name\n",
    "model_name = 'lstm_'+'.h5'\n",
    "\n",
    "\n",
    "train_data_X = X_data\n",
    "train_data_y = y_data\n",
    "\n",
    "# split train and test\n",
    "\n",
    "# traininig \n",
    "size_df = X_data.shape[0]\n",
    "train_size_percent = train_size_percent/100\n",
    "train_size = int(round(size_df*train_size_percent,0))\n",
    "x_train = X_data[:train_size,:]\n",
    "y_train = y_data[:train_size,:]\n",
    "\n",
    "# testing\n",
    "\n",
    "x_test = X_data[train_size:,:]\n",
    "y_test = y_data[train_size:,:]\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "#cross validation\n",
    "\n",
    "\n",
    "# for train_index, test_index in tscv.split(train_data_X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     x_train, x_test = train_data_X[train_index], train_data_X[test_index]\n",
    "#     y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "# hyperparameters to dictionary\n",
    "dictionary = {\n",
    "\"lags\": lag,\n",
    "\"n_fut\": n_fut,\n",
    "\"n_hidden_layers\": n_hidden_layers,\n",
    "\"batch_size\": batch_size,\n",
    "\"units\": units,\n",
    "\"dropout\": dropout,\n",
    "\"epochs\": epochs,\n",
    "\"learning_rate\": learning_rate,\n",
    "\"reg_l1\": l1,\n",
    "\"reg_l2\": l2,\n",
    "\"features\":df_colnames,  \n",
    "}\n",
    "mlflow.log_param('dict',dictionary)\n",
    "\n",
    "print('The hyperparameters for the current experiments:')\n",
    "print(dictionary)\n",
    "# dump all the hyperparameters in to a dictionary and save to .json file\n",
    "hyperparms(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if MODEL_NAME == 'LSTM':\n",
    "\n",
    "# initializing model\n",
    "model_init = LSTM_model(n_hidden_layers, units, dropout, x_train, y_train, epochs, reg)\n",
    "\n",
    "# calling the model\n",
    "model = model_init.build_model()\n",
    "model.build((x_train.shape[0],x_train.shape[1], x_train.shape[2]))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics for evaluating the model\n",
    "\n",
    "\n",
    "\n",
    "metrics = [tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanAbsolutePercentageError()]\n",
    "\n",
    "# model compiler\n",
    "model.compile(optimizer=Adam(learning_rate = learning_rate), loss='mse', metrics = metrics)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#setting the callback function\n",
    "cb = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(path_checkpoint),\n",
    "    tf.keras.callbacks.CSVLogger(path_metrics+'/'+'data.csv')]\n",
    "    #tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=, restore_best_weights=False)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model fitting protocol\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = epochs, \n",
    "                    batch_size = batch_size,  \n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose = 1,\n",
    "                    callbacks=[cb],\n",
    "                    shuffle= False,\n",
    "                    workers = 8,\n",
    "                    use_multiprocessing = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataset\n",
    "train_loss, train_RMSE, train_MAE, train_MAPE = model.evaluate(x_train, y_train)\n",
    "\n",
    "print('\\n','Evaluation of Training dataset:','\\n''\\n','train_loss:',round(train_loss,3),'\\n','train_RMSE:',round(train_RMSE,3),'\\n', 'train_MAE:',round(train_MAE,3),'\\n','train_MAPE:',round(train_MAPE,3))\n",
    "\n",
    "\n",
    "#val dataset\n",
    "val_loss, val_RMSE, val_MAE, val_MAPE = model.evaluate(x_test, y_test)\n",
    "print('\\n','Evaluation of Testing dataset:','\\n''\\n','val_loss:',round(val_loss,3),'\\n','val_RMSE:',round(val_RMSE,3),'\\n', 'val_MAE:',round(val_MAE,3),'\\n','val_MAPE:',round(val_MAPE,3))\n",
    "\n",
    "# predict on val set\n",
    "\n",
    "\n",
    "\n",
    "mlflow.log_metric('train_loss',train_loss)\n",
    "mlflow.log_metric('RMSE', train_RMSE)\n",
    "mlflow.log_metric('MAE', train_MAE)\n",
    "mlflow.log_metric('MAPE',train_MAPE)\n",
    "\n",
    "mlflow.log_metric('val_loss',val_loss)\n",
    "mlflow.log_metric('val_RMSE', val_RMSE)\n",
    "mlflow.log_metric('val_MAE', val_MAE)\n",
    "mlflow.log_metric('val_MAPE',val_MAPE)\n",
    "\n",
    "mlflow.keras.save_model(model, path =path_model+'/')\n",
    "\n",
    "path_metrics+'/'+'data.csv'\n",
    "df = pd.read_csv(path_metrics+'/'+'data.csv')\n",
    "\n",
    "metricplot(df, 'epoch', 'loss','val_loss', path_metrics)\n",
    "metricplot(df, 'epoch', 'mean_absolute_error','val_mean_absolute_error', path_metrics)\n",
    "metricplot(df, 'epoch', 'mean_absolute_percentage_error','val_mean_absolute_percentage_error', path_metrics)\n",
    "metricplot(df, 'epoch', 'root_mean_squared_error','val_root_mean_squared_error', path_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = model.predict(x_test)\n",
    "MAPE_pred = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "print(\"The MAPE on the test data is:\",round(MAPE_pred(y_test, y_test_pred).numpy(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_model+'/'+model_name)   \n",
    "#model_name = 'lstm_128.h5'\n",
    "#path_model_tr =\"../Model_Outputs/2022_11_01/best_fit5/model_Bilstm/model\" \n",
    "model_eval = load_model(path_model+'/'+model_name, compile=False)\n",
    "\n",
    "# get future dates and predict the future close price!\n",
    "future_days = 10\n",
    "\n",
    "startdate = list(df_datetime['date'])[-1]\n",
    "startdate = pd.to_datetime(startdate) + pd.DateOffset(days=1)\n",
    "enddate = pd.to_datetime(startdate) + pd.DateOffset(days=future_days+1)\n",
    "forecasting_dates= pd.bdate_range(start=startdate, end=enddate, freq = 'B')\n",
    "number_of_days = len(forecasting_dates)\n",
    "forecast = model_eval.predict(train_data_X[-len(forecasting_dates):])\n",
    "forecast_copies = np.repeat(forecast, df_data.shape[1], axis = -1 )\n",
    "y_pred_fut = scaler.inverse_transform(forecast_copies)[:,0]\n",
    "forecast_close = {'dates':forecasting_dates ,'close': y_pred_fut, 'close_orig':data['close'].iloc[-number_of_days:] }\n",
    "forecasting_df = pd.DataFrame(data = forecast_close)\n",
    "forecasting_df.to_csv(path_forecast +'/'+ 'forecast.csv')\n",
    "print('The forecast for the future',number_of_days,'days is:','\\n',forecasting_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_pred = model.predict(x_test)\n",
    "MAPE_pred = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "print(\"The MAPE on the test data is:\",round(MAPE_pred(forecasting_df['close'], forecasting_df['close_orig']).numpy(),3),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecasting_df\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot(data = forecasting_df, x  = forecasting_df['dates'], y = forecasting_df['close'], label='close_pred')\n",
    "sns.scatterplot(data = forecasting_df, x  = forecasting_df['dates'], y = forecasting_df['close'])\n",
    "\n",
    "sns.lineplot(data = forecasting_df, x = forecasting_df['dates'], y = forecasting_df['close_orig'],label='close_actual')\n",
    "sns.scatterplot(data = forecasting_df, x = forecasting_df['dates'], y = forecasting_df['close_orig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('scaler_std.bin', 'wb') as f_out:\n",
    "    pickle.dump((scaler), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39410d665ad54f46fb296cb711c5a6cfcd8aeec8eb6a5a752176048eef25518b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
