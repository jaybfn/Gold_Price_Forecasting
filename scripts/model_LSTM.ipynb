{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.regularizers import L1L2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create all the necessary directory!\n",
    "\n",
    "def create_dir(path):\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path, exist_ok = False)\n",
    "            print(\"New directory is created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Jayesh_ML\\Gold_Price_Forecasting\\scripts\\model_LSTM.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dictionary \u001b[39m=\u001b[39m {\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlags\u001b[39m\u001b[39m\"\u001b[39m: lag,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn_hidden_layers\u001b[39m\u001b[39m\"\u001b[39m: n_hidden_layers,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: batch_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39munits\u001b[39m\u001b[39m\"\u001b[39m: units,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m\"\u001b[39m: dropout,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m: epochs,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: learning_rate,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreg_l1\u001b[39m\u001b[39m\"\u001b[39m: l1,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreg_l2\u001b[39m\u001b[39m\"\u001b[39m: l2  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhyperparms\u001b[39m(dictionary):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Serializing json\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     json_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(dictionary, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lag' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary = {\n",
    "    \"lags\": lag,\n",
    "    \"n_hidden_layers\": n_hidden_layers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"units\": units,\n",
    "    \"dropout\": dropout,\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"reg_l1\": l1,\n",
    "    \"reg_l2\": l2  \n",
    "}\n",
    "\n",
    "def hyperparms(dictionary):\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(dictionary, indent=4)\n",
    "    \n",
    "    # Writing to sample.json\n",
    "    with open(path_metrics +'/'+ 'hyperparm.json', \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_split(data, y_data, train_split=0.7):\n",
    "    \n",
    "#     \"\"\" This function will split the dataframe into training and testing set.\n",
    "#     Inputs: data: Pandas DatFrame\n",
    "#             train_split: default is set to 0.9. Its a ratio to split the trining and testing datset.\n",
    "#     \"\"\"\n",
    "#     split = int(train_split*len(data)) # for training\n",
    "#     split_test = int(0.90*len(data))\n",
    "#     X_train = data[:split]\n",
    "#     y_train = y_data[:split]\n",
    "#     X_val = data[split:split_test]\n",
    "#     y_val = y_data[split:split_test]\n",
    "#     X_test = data[split_test:]\n",
    "#     y_test = y_data[split_test:]\n",
    "\n",
    "#     return X_train, y_train, X_val, y_val,  X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and format data\n",
    "\n",
    "class DataFormatting():\n",
    "      \n",
    "    def __init__(self):\n",
    "       \n",
    "        self.df_data = None\n",
    "        self.df_datetime = None\n",
    "\n",
    "    def dataset(df):\n",
    "\n",
    "        # converting time colum from object type to datetime format\n",
    "        df['date'] = pd.to_datetime(df['date'],dayfirst = True, format = '%Y-%m-%d')\n",
    "        # creating a ema feature\n",
    "        df['SMA_10'] = df[['close']].rolling(10).mean().shift(1)\n",
    "        df = df.dropna()\n",
    "        # splitting the dataframe in to X and y \n",
    "        df_data = df[['open','close','high','low','SMA_10']]#,\n",
    "        cols = list(df_data[1:6])\n",
    "        df_data = df_data[cols].astype(float)\n",
    "        df_datetime =df[['date']]\n",
    "\n",
    "        return df_data, df_datetime\n",
    "\n",
    "\n",
    "# split the dataset in tain and test!\n",
    "\n",
    "def train_test_split(data, y_data, train_split=0.7):\n",
    "    \n",
    "    \"\"\" This function will split the dataframe into training and testing set.\n",
    "    Inputs: data: Pandas DatFrame\n",
    "            train_split: default is set to 0.9. Its a ratio to split the trining and testing datset.\n",
    "    \"\"\"\n",
    "    split = int(train_split*len(data)) # for training\n",
    "    #split_test = int(0.90*len(data))\n",
    "    X_train = data[:split]\n",
    "    y_train = y_data[:split]\n",
    "    X_val = data[split:]\n",
    "    y_val = y_data[split:]\n",
    "    # X_test = data[split_test:]\n",
    "    # y_test = y_data[split_test:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "# Data transformation (changing data shape to model requirement)\n",
    "\n",
    "def data_transformation(data, lags = 5):\n",
    "    \n",
    "    \"\"\" this function transforms dataframe to required input shape for the model.\n",
    "    It required 2 input arguments:\n",
    "    1. data: this will be the pandas dataframe\n",
    "    2. lags: how many previous price points to be used to predict the next future value, in\n",
    "    this case the default is set to 5 for 'XAUUSD' commodity\"\"\"\n",
    "\n",
    "    # initialize lists to store the dataset\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(lags, len(data)):\n",
    "        X_data.append(data[i-lags: i, 0: data.shape[1]])\n",
    "        y_data.append(data[i,1:2]) # extracts close price with specific lag as price to be predicted.\n",
    "\n",
    "    # convert the list to numpy array\n",
    "\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return X_data, y_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building\n",
    "\n",
    "class LSTM_model():\n",
    "    \n",
    "\n",
    "    def __init__(self,n_hidden_layers, units, dropout, train_data_X, train_data_y, epochs, reg):\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.train_data_X = train_data_X\n",
    "        self.train_data_y = train_data_y\n",
    "        self.epochs = epochs\n",
    "        self.reg = reg\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        # first lstm layer\n",
    "        model.add(LSTM(self.units, activation='tanh', input_shape=(self.train_data_X.shape[1], self.train_data_X.shape[2]), kernel_regularizer=self.reg, return_sequences=True))\n",
    "\n",
    "        if self.n_hidden_layers !=1:\n",
    "\n",
    "            # building hidden layers\n",
    "            for i in range(1, self.n_hidden_layers):\n",
    "                # for the last layer as the return sequence is False\n",
    "                if i == self.n_hidden_layers -1:\n",
    "                    model.add(LSTM(int(self.units/(2**i)),  activation='tanh', return_sequences=False))\n",
    "                else:\n",
    "                    model.add(LSTM(int(self.units/(2**i)),  activation='tanh', return_sequences=True))\n",
    "\n",
    "        else:\n",
    "            model.add(LSTM(int(self.units/(2)),  activation='tanh', return_sequences=False))\n",
    "\n",
    "        # adding dropout layer\n",
    "        model.add(Dropout(self.dropout))\n",
    "        # final layer\n",
    "        model.add(Dense(self.train_data_y.shape[1]))\n",
    "\n",
    "        \n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricplot(df, xlab, ylab_1,ylab_2, path):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function plots metric curves and saves it\n",
    "    to respective folder\n",
    "    inputs: df : pandas dataframe \n",
    "            xlab: x-axis\n",
    "            ylab_1 : yaxis_1\n",
    "            ylab_2 : yaxis_2\n",
    "            path: full path for saving the plot\n",
    "            \"\"\"\n",
    "    plt.figure()\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    sns.lineplot(x = df[xlab], y = df[ylab_1])\n",
    "    sns.lineplot(x = df[xlab], y = df[ylab_2])\n",
    "    plt.xlabel('Epochs',fontsize = 12)\n",
    "    plt.ylabel(ylab_1,fontsize = 12)\n",
    "    plt.xticks(fontsize = 12)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    plt.legend([ylab_1,ylab_2], prop={\"size\":12})\n",
    "    plt.savefig(path+'/'+ ylab_1)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[nan] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Jayesh_ML\\Gold_Price_Forecasting\\scripts\\model_LSTM.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m j \u001b[39m=\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/gold_mt5.csv\u001b[39m\u001b[39m'\u001b[39m,index_col\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m]) \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data\u001b[39m.\u001b[39;49mdrop(index\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mindex[\u001b[39m-\u001b[39;49mj:],axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m# dropping rows from bottom for forecasting\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m seed(\u001b[39m42\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mset_seed(\u001b[39m42\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\deepL\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\deepL\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4806\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   4807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   4808\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4816\u001b[0m ):\n\u001b[0;32m   4817\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4952\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4954\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   4955\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   4956\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   4957\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   4958\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   4959\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   4960\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   4961\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   4962\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\deepL\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4269\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\deepL\\lib\\site-packages\\pandas\\core\\generic.py:4340\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4338\u001b[0m     labels_missing \u001b[39m=\u001b[39m (axis\u001b[39m.\u001b[39mget_indexer_for(labels) \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39many()\n\u001b[0;32m   4339\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m labels_missing:\n\u001b[1;32m-> 4340\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4342\u001b[0m \u001b[39mif\u001b[39;00m is_extension_array_dtype(mask\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m   4343\u001b[0m     \u001b[39m# GH#45860\u001b[39;00m\n\u001b[0;32m   4344\u001b[0m     mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mto_numpy(dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: '[nan] not found in axis'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "     # loading the dataset!\n",
    "    \n",
    "     for i in range(10):\n",
    "          j = i+1\n",
    "          data = pd.read_csv('../data/gold_mt5.csv',index_col=[0]) \n",
    "          data.drop(index=data.index[-j:],axis=0, inplace=True) # dropping rows from bottom for forecasting\n",
    "\n",
    "          seed(42)\n",
    "          tf.random.set_seed(42) \n",
    "          keras.backend.clear_session()\n",
    "\n",
    "          # model hyperparameters!\n",
    "          lag =5\n",
    "          n_hidden_layers = 2\n",
    "          batch_size = 64 #256\n",
    "          units = 32\n",
    "          dropout = 0.2\n",
    "          epochs = 100\n",
    "          learning_rate = 0.01\n",
    "          reg = L1L2(l1=0.03, l2=0.01)\n",
    "          #print(dictionary)\n",
    "          # dump all the hyperparameters in to a disctionary and save to .json file\n",
    "          #hyperparms(dictionary)\n",
    "\n",
    "          # creating main folder\n",
    "          today = datetime.now()\n",
    "          today  = today.strftime('%Y_%m_%d')\n",
    "          path = '../Model_Outputs/'+ today\n",
    "          create_dir(path)\n",
    "\n",
    "          # creating directory to save model and its output\n",
    "          folder = 'model_lstm'+ str(units) + '_' + str(n_hidden_layers) + '_' + str(j)\n",
    "          path_main = path + '/'+ folder\n",
    "          create_dir(path_main)\n",
    "\n",
    "          # creating directory to save all the metric data\n",
    "          folder = 'metrics'\n",
    "          path_metrics = path_main +'/'+ folder\n",
    "          create_dir(path_metrics)\n",
    "\n",
    "          # creating folder to save model.h5 file\n",
    "          folder = 'model'\n",
    "          path_model = path_main +'/'+ folder\n",
    "          create_dir(path_model)\n",
    "\n",
    "          # creating folder to save model.h5 file\n",
    "          folder = 'model_checkpoint'\n",
    "          path_checkpoint = path_main +'/'+ folder\n",
    "          create_dir(path_checkpoint)\n",
    "\n",
    "\n",
    "          # initializing DataFormatting class\n",
    "          data_init = DataFormatting()\n",
    "          df_data, df_datetime = DataFormatting.dataset(data)\n",
    "          print('\\n')\n",
    "          print('Displaying top 5 rows of the dataset:')\n",
    "          print('\\n')\n",
    "          print(df_data.tail())\n",
    "          print(df_datetime)\n",
    "\n",
    "          # normalize train, val and test dataset\n",
    "\n",
    "          # initialize StandartScaler()\n",
    "          scaler = StandardScaler()\n",
    "          scaler = scaler.fit(df_data)\n",
    "          data_fit_transformed = scaler.transform(df_data)\n",
    "\n",
    "          print('\\n')\n",
    "          print('Displaying top 5 rows of all the scaled dataset:')\n",
    "          print('\\n')\n",
    "          print('The train dateset:','\\n''\\n',data_fit_transformed[0:5])\n",
    "\n",
    "\n",
    "          # changing shape of the data to match the model requirement!\n",
    "\n",
    "          X_data, y_data = data_transformation(data_fit_transformed, lags = lag)\n",
    "          print('\\n')\n",
    "          print('Displaying the shape of the dataset required by the model:')\n",
    "          print('\\n')\n",
    "          print(f' Input shape X:',X_data.shape, f'Input shape y:',y_data.shape)\n",
    "          print('\\n')\n",
    "          print(X_data, y_data)\n",
    "          print(X_data.shape, X_data.shape[1])\n",
    "\n",
    "          # create train test split\n",
    "          X_train, y_train, X_val, y_val = train_test_split(X_data, y_data, train_split=0.9)\n",
    "\n",
    "          # print( X_test)\n",
    "\n",
    "          # input data\n",
    "          train_data_X = X_train\n",
    "          train_data_y = y_train\n",
    "          print(train_data_X)\n",
    "          print(train_data_y)\n",
    "\n",
    "          print(train_data_X.shape)\n",
    "          print(train_data_y.shape)\n",
    "\n",
    "          print(X_val.shape)\n",
    "          print(y_val.shape)\n",
    "          # # initializing model\n",
    "          # model_init = LSTM_model(n_hidden_layers, units, dropout, train_data_X, train_data_y, epochs, reg)\n",
    "\n",
    "          # # calling the model\n",
    "          # model = model_init.build_model()\n",
    "          # print(model.summary())\n",
    "          # # metrics for evaluating the model\n",
    "          # metrics = [tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanAbsolutePercentageError()]\n",
    "\n",
    "          # # model compiler\n",
    "          # model.compile(optimizer=Adam(learning_rate = learning_rate), loss='mse', metrics = metrics)\n",
    "\n",
    "          # # setting the model file name\n",
    "          # model_name = 'lstm_'+ str(units)+'.h5'\n",
    "\n",
    "          # # setting the callback function\n",
    "          # cb = [\n",
    "          #     tf.keras.callbacks.ModelCheckpoint(path_checkpoint),\n",
    "          #     tf.keras.callbacks.CSVLogger(path_metrics+'/'+'data.csv'),\n",
    "          #     tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1001, restore_best_weights=False)]\n",
    "\n",
    "          # # model fitting protocol\n",
    "          # history = model.fit(train_data_X,train_data_y, \n",
    "          #                     epochs = epochs, \n",
    "          #                     batch_size = batch_size, \n",
    "          #                     validation_data=(X_val_data, y_val_data), \n",
    "          #                     verbose = 1,\n",
    "          #                     callbacks=[cb],\n",
    "          #                     shuffle= False)\n",
    "\n",
    "          # # Model evaluation\n",
    "\n",
    "          # # training dataset\n",
    "          # train_loss, RMSE, MAE, MAPE = model.evaluate(train_data_X,train_data_y)\n",
    "          # print('\\n','Evaluation of Training dataset:','\\n''\\n','train_loss:',round(train_loss,3),'\\n','RMSE:',round(RMSE,3),'\\n', 'MAE:',round(MAE,3),'\\n','MAPE:',round(MAPE,3))\n",
    "\n",
    "          # # validation dataset\n",
    "          # val_loss, val_RMSE, val_MAE, val_MAPE = model.evaluate(X_val_data, y_val_data)\n",
    "          # print('\\n','Evaluation of Validation dataset:','\\n''\\n','val_loss:',round(val_loss,3),'\\n','val_RMSE:',round(val_RMSE,3),'\\n', 'val_MAE:',round(val_MAE,3),'\\n','MAPE:',round(MAPE,3))\n",
    "          # # path to save model\n",
    "\n",
    "          # model.save(path_model+'/'+model_name)   \n",
    "\n",
    "          # path_metrics+'/'+'data.csv'\n",
    "          # df = pd.read_csv(path_metrics+'/'+'data.csv')\n",
    "\n",
    "          # metricplot(df, 'epoch', 'loss','val_loss', path_metrics)\n",
    "          # metricplot(df, 'epoch', 'mean_absolute_error','val_mean_absolute_error', path_metrics)\n",
    "          # metricplot(df, 'epoch', 'mean_absolute_percentage_error','val_mean_absolute_percentage_error', path_metrics)\n",
    "          # metricplot(df, 'epoch', 'root_mean_squared_error','val_root_mean_squared_error', path_metrics)\n",
    "\n",
    "\n",
    "\n",
    "          # model_name = 'lstm_'+ str(units)+'.h5'\n",
    "          # model_eval = load_model(path_model+'/'+model_name, compile=False)\n",
    "\n",
    "          # # prediction on the test set\n",
    "\n",
    "          # y_pred_close = model_eval.predict(X_test_data)\n",
    "          # y_pred_close_copies = np.repeat(y_pred_close, X_train.shape[1], axis = -1)\n",
    "          # #print(y_pred_close_copies)\n",
    "          # y_pred_scaled = scaler.inverse_transform(y_pred_close_copies)[:,0]\n",
    "\n",
    "          # y_test_true = np.repeat(y_test_data, X_train.shape[1], axis = -1)\n",
    "          # y_test_scaled = scaler.inverse_transform(y_test_true)[:,0]\n",
    "          # print(y_test_scaled[:10])\n",
    "          # print(y_pred_scaled[:10])\n",
    "          # # lets compare the predicted output and the original values using RMSE as a metric\n",
    "          # RMSE_test = sqrt(mean_squared_error(y_test_scaled, y_pred_scaled))\n",
    "          # #rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "          # #RMSE_test = rmse(y_test_scaled, y_pred_scaled)\n",
    "          # print('\\n')\n",
    "          # print('The RMSE on the test data set is:',RMSE_test)\n",
    "          # print('\\n')\n",
    "\n",
    "          future_days = 5\n",
    "\n",
    "          # forecast_date = pd.date_range(list(df_datetime['date'])[-1], periods = future_days, freq = '1D').tolist()\n",
    "          # print(forecast_date)\n",
    "          # date = df_datetime.iloc[:,0]\n",
    "          # startdate = date.iloc[-1].\n",
    "          # print(startdate)\n",
    "          startdate = list(df_datetime['date'])[-1]\n",
    "          #print(startdate)\n",
    "          startdate = pd.to_datetime(startdate, dayfirst = True, format = '%d-%m-%Y') + pd.DateOffset(days=1)\n",
    "          print(startdate)\n",
    "          enddate = pd.to_datetime(startdate, dayfirst = True, format = '%d-%m-%Y') + pd.DateOffset(days=future_days)\n",
    "          forecasting_dates= pd.bdate_range(start=startdate, end=enddate, freq = 'B')\n",
    "          print(forecasting_dates)\n",
    "          # forecast = model_eval.predict(X_data[-future_days:])\n",
    "          # #print(forecast)\n",
    "          # forecast_copies = np.repeat(forecast, X_data.shape[2], axis = -1 )\n",
    "          # #print(forecast_copies)\n",
    "          # y_pred_fut = scaler.inverse_transform(forecast_copies)[:,0]\n",
    "          # print('The forecast for the future 10 days is:','\\n',y_pred_fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Jayesh_ML\\Gold_Price_Forecasting\\scripts\\model_LSTM.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m startdate \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(df_datetime[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m])[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m startdate \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(startdate) \u001b[39m+\u001b[39m pd\u001b[39m.\u001b[39mDateOffset(days\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Jayesh_ML/Gold_Price_Forecasting/scripts/model_LSTM.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m enddate \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(startdate) \u001b[39m+\u001b[39m pd\u001b[39m.\u001b[39mDateOffset(days\u001b[39m=\u001b[39mfuture_days\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_datetime' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "startdate = list(df_datetime['date'])[-1]\n",
    "startdate = pd.to_datetime(startdate) + pd.DateOffset(days=1)\n",
    "enddate = pd.to_datetime(startdate) + pd.DateOffset(days=future_days+1)\n",
    "forecasting_dates= pd.bdate_range(start=startdate, end=enddate, freq = 'B')\n",
    "print(len(forecasting_dates))\n",
    "dates =  {'dates':forecasting_dates }\n",
    "forecasting_df = pd.DataFrame(data = dates)\n",
    "print(forecasting_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters!\n",
    "lag =5\n",
    "n_hidden_layers = 2\n",
    "batch_size = 64 #256\n",
    "units = 32\n",
    "dropout = 0.2\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "l1 = 0.03\n",
    "l2 = 0.01\n",
    "reg = L1L2(l1=l1, l2=l2)\n",
    "\n",
    "\n",
    "dictionary = {\n",
    "    \"lags\": lag,\n",
    "    \"n_hidden_layers\": n_hidden_layers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"units\": units,\n",
    "    \"dropout\": dropout,\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"reg_l1\": l1,\n",
    "    \"reg_l2\": l2  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparms(dictionary):\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(dictionary, indent=4)\n",
    "    \n",
    "    # Writing to sample.json\n",
    "    with open(path_metrics +'/'+ 'hyperparm.json', \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparms(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing json\n",
    "json_object = json.dumps(dictionary, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(path_metrics +'/'+ 'hyperparm.json', \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=define_model, \n",
    "                        verbose=1)\n",
    "\n",
    "dropout_rate = [0.0, 0.2, 0.4]\n",
    "weight_constraint = [1, 2, 3]\n",
    "neurons = [10, 32, 64]\n",
    "batch_size = [100, 200, 400]\n",
    "epochs = [1, 5, 10]\n",
    "\n",
    "param_grid = dict(dropout_rate=dropout_rate, \n",
    "                  weight_constraint=weight_constraint,\n",
    "                  neurons=neurons, batch_size=batch_size, \n",
    "                  epochs=epochs)\n",
    "\n",
    "#n_jobs=16 uses 16 CPUs. Try not to do -1 on your system as it may hang!!!\n",
    "# -1 refers to using all available CPUs\n",
    "#Cross validation, cv=3\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=16, cv=3)\n",
    "\n",
    "grid_result = grid.fit(x_grid, y_grid)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean = %f (std=%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('deepL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b90cab7ea642421f44636989edaf96d86cb1abe354b45ce6eed3b362842c2584"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
