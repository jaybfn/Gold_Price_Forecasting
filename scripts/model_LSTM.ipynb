{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFormatting():\n",
    "  \n",
    "    def __init__(self):\n",
    "       \n",
    "        self.df_data = None\n",
    "        self.df_datetime = None\n",
    "\n",
    "    def dataset(df):\n",
    "\n",
    "        # converting time colum from object type to datetime format\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        # splitting the dataframe in to X and y \n",
    "        df_data = df[['open','high','low','close','tick_volume']]\n",
    "        df_datetime =df[['time']]\n",
    "\n",
    "        return df_data, df_datetime\n",
    "\n",
    "\n",
    "data = pd.read_csv('../data/gold_mt5.csv',index_col=[0]) \n",
    "\n",
    "data_init = DataFormatting()\n",
    "df_data, _ = DataFormatting.dataset(data)\n",
    "print(df_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_split=0.9):\n",
    "\n",
    "    \"\"\" This function will split the dataframe into training and testing set.\n",
    "    Inputs: data: Pandas DatFrame\n",
    "            train_split: default is set to 0.9. Its a ratio to split the trining and testing datset.\n",
    "    \"\"\"\n",
    "    split = int(train_split*len(data)) # for training\n",
    "    X_train = data.iloc[:split,:]\n",
    "    X_test = data.iloc[split:,:]\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "X_train, X_test = train_test_split(df_data, train_split=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize():\n",
    "\n",
    "    \"\"\" class Normalize uses standard scaler method to normalize the dataset\"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        self.data_fit_transformed = None\n",
    "        self.data_inverse_transformed = None\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "\n",
    "        # initialize StandartScaler()\n",
    "        scaler = StandardScaler()\n",
    "        # fit the method on the dataset\n",
    "        scaler = scaler.fit(data)\n",
    "        # transform the dataset\n",
    "        data_fit_transformed = scaler.transform(data)\n",
    "\n",
    "        return data_fit_transformed\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "\n",
    "        # initialize StandartScaler()\n",
    "        scaler = StandardScaler()\n",
    "        # inverse transform the dataset\n",
    "        data_inverse_transformed = scaler.inverse_transform(data)\n",
    "        \n",
    "        return data_inverse_transformed\n",
    "\n",
    "# normalize\n",
    "scaler_init = Normalize()\n",
    "scaled_data = scaler_init.fit_transform(X_train)\n",
    "print(scaled_data[0:11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation(data, lags = 5):\n",
    "\n",
    "    \"\"\" this function transforms dataframe to required input shape for the model.\n",
    "    It required 2 input arguments:\n",
    "    1. data: this will be the pandas dataframe\n",
    "    2. lags: how many previous price points to be used to predict the next future value, in\n",
    "    this case the default is set to 5 for 'XAUUSD' commodity\"\"\"\n",
    "\n",
    "    # initialize lists to store the dataset\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(lags, len(data)):\n",
    "        X_data.append(data[i-lags: i, 0: data.shape[1]])\n",
    "        y_data.append(data[i,3:4]) # extracts close price with specific lag as price to be predicted.\n",
    "\n",
    "    # convert the list to numpy array\n",
    "\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return X_data, y_data\n",
    "\n",
    "\n",
    "X_data, y_data = data_transformation(scaled_data, lags = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_data.shape[0],X_data.shape[1],X_data.shape[2],y_data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model():\n",
    "    seed(42)\n",
    "    tf.random.set_seed(42) \n",
    "\n",
    "    def __init__(self,n_hidden_layers, units, dropout, train_data_X, train_data_y, epochs):\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.train_data_X = train_data_X\n",
    "        self.train_data_y = train_data_y\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        # first lstm layer\n",
    "        model.add(LSTM(self.units, activation='relu', input_shape=(self.train_data_X.shape[1], self.train_data_X.shape[2]), return_sequences=True))\n",
    "        # building hidden layers\n",
    "        for i in range(1, self.n_hidden_layers):\n",
    "            # for the last layer as the return sequence is False\n",
    "            if i == self.n_hidden_layers -1:\n",
    "                model.add(LSTM(int(self.units/(2**i)),  activation='relu', return_sequences=False))\n",
    "            else:\n",
    "                model.add(LSTM(int(self.units/(2**i)),  activation='relu', return_sequences=True))\n",
    "        # adding droupout layer\n",
    "        model.add(Dropout(self.dropout))\n",
    "        # final layer\n",
    "        model.add(Dense(self.train_data_y.shape[1]))\n",
    "        return model\n",
    "        #model.summary()\n",
    "\n",
    "    def fit_lstm(self):\n",
    "\n",
    "        lstm_model = self.build_model()\n",
    "        metrics = [tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanAbsolutePercentageError()]\n",
    "        lstm_model.compile(optimizer=Adam(learning_rate = 0.0001), loss='mse', metrics = metrics)\n",
    "        \n",
    "        model_name = 'lstm_'+ str(self.units)\n",
    "        path_model = f\"../Model_Outputs/model_lstm\"\n",
    "        path_metric = f\"../Model_Outputs/model_lstm\"\n",
    "        cb = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(path_model+'/'+model_name),\n",
    "            tf.keras.callbacks.CSVLogger(path_metric+'/'+'data.csv'),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)]\n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "        history = lstm_model.fit(self.train_data_X,self.train_data_y, \n",
    "                            epochs = self.epochs, \n",
    "                            batch_size = 8, \n",
    "                            validation_split=0.2, \n",
    "                            verbose = 1,\n",
    "                            callbacks=[cb],\n",
    "                            shuffle= False)\n",
    "\n",
    "\n",
    "n_hidden_layers = 3\n",
    "units = 128\n",
    "dropout = 0.2\n",
    "train_data_X = X_data \n",
    "train_data_y = y_data\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "model = LSTM_model(n_hidden_layers, units, dropout, train_data_X, train_data_y, epochs)\n",
    "\n",
    "summary = model.fit_lstm()\n",
    "print(summary)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricplot(df, xlab, ylab_1,ylab_2, path):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function plots metric curves and saves it\n",
    "    to respective folder\n",
    "    inputs: df : pandas dataframe \n",
    "            xlab: x-axis\n",
    "            ylab_1 : yaxis_1\n",
    "            ylab_2 : yaxis_2\n",
    "            path: full path for saving the plot\n",
    "            \"\"\"\n",
    "    plt.figure()\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    sns.lineplot(x = df[xlab], y = df[ylab_1])\n",
    "    sns.lineplot(x = df[xlab], y = df[ylab_2])\n",
    "    plt.xlabel('Epochs',fontsize = 12)\n",
    "    plt.ylabel(ylab_1,fontsize = 12)\n",
    "    plt.xticks(fontsize = 12)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    plt.legend([ylab_1,ylab_2], prop={\"size\":12})\n",
    "    plt.savefig(path+'/'+ ylab_1)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_met = pd.read_csv('../Model_Outputs/model_lstm/data.csv')\n",
    "data_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Model_Outputs/model_lstm'\n",
    "df = pd.read_csv('../Model_Outputs/model_lstm/data.csv')\n",
    "\n",
    "metricplot(df, 'epoch', 'loss','val_loss', path)\n",
    "metricplot(df, 'epoch', 'mean_absolute_error','val_mean_absolute_error', path)\n",
    "metricplot(df, 'epoch', 'mean_absolute_percentage_error','val_mean_absolute_percentage_error', path)\n",
    "metricplot(df, 'epoch', 'root_mean_squared_error','val_root_mean_squared_error', path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('deepL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b90cab7ea642421f44636989edaf96d86cb1abe354b45ce6eed3b362842c2584"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
