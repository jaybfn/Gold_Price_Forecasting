{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 13:04:11.789614: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 13:04:11.882359: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-13 13:04:11.882374: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-13 13:04:11.900809: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-13 13:04:12.397839: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 13:04:12.397895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 13:04:12.397901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFormatting():\n",
    "      \n",
    "    def __init__(self):\n",
    "        self.df_data = None\n",
    "        self.df_datetime = None\n",
    "       \n",
    "    def dataset(df):\n",
    "\n",
    "        # converting time colum from object type to datetime format\n",
    "        df['date'] = pd.to_datetime(df['date'],dayfirst = True, format = '%d/%m/%Y')\n",
    "        df = df.dropna()\n",
    "        # splitting the dataframe in to X and y \n",
    "        df_data = df[['open','high','low','close']] #'high','low',,'CRUDE_OIL_CLOSE','US500_CLOSE','open','EXCHANGE_RATE',\n",
    "        df_datetime =df[['date']]\n",
    "\n",
    "        return df_data, df_datetime\n",
    "\n",
    "\n",
    "# Data transformation (changing data shape to model requirement)\n",
    "\n",
    "def data_transformation(data, lags = 5, n_fut = 1):\n",
    "    \n",
    "    \"\"\" this function transforms dataframe to required input shape for the model.\n",
    "    It required 2 input arguments:\n",
    "    1. data: this will be the pandas dataframe\n",
    "    2. lags: how many previous price points to be used to predict the next future value, in\n",
    "    this case the default is set to 5 for 'EURUSD' commodity\"\"\"\n",
    "\n",
    "    # initialize lists to store the dataset\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(lags, len(data)- n_fut +1):\n",
    "        X_data.append(data[i-lags: i, 0: data.shape[1]])\n",
    "        y_data.append(data[i+ n_fut-1:i+n_fut,3]) # extracts close price with specific lag as price to be predicted.\n",
    "\n",
    "    # convert the list to numpy array\n",
    "\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 5\n",
    "n_fut = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Displaying top 5 rows of the dataset:\n",
      "\n",
      "\n",
      "         open     high      low    close\n",
      "9994  0.98734  0.99768  0.98486  0.99678\n",
      "9995  0.99610  1.00886  0.99437  1.00833\n",
      "9996  1.00783  1.00938  0.99576  0.99649\n",
      "9997  0.99642  0.99982  0.99268  0.99658\n",
      "9998  0.99465  0.99658  0.98729  0.98825\n",
      "(6, 4)\n",
      "Index(['open', 'high', 'low', 'close'], dtype='object')\n",
      "\n",
      "\n",
      "Displaying top 5 rows of all the scaled dataset:\n",
      "\n",
      "\n",
      "The train dateset: \n",
      "\n",
      " [[-0.96239694 -0.92667316 -0.95543531 -0.91028885]\n",
      " [-0.91363094 -0.86484    -0.90213312 -0.84598282]\n",
      " [-0.84833127 -0.86196404 -0.89434236 -0.91190347]\n",
      " [-0.91184954 -0.91483747 -0.91160532 -0.91140238]\n",
      " [-0.92170294 -0.93275692 -0.9418155  -0.95778067]\n",
      " [-0.96295363 -0.94791102 -0.94231994 -0.93300475]]\n",
      "\n",
      "\n",
      "Displaying the shape of the dataset required by the model:\n",
      "\n",
      "\n",
      " Input shape X: (1, 5, 4) Input shape y: (1, 1)\n",
      "\n",
      "\n",
      "[[[-0.96239694 -0.92667316 -0.95543531 -0.91028885]\n",
      "  [-0.91363094 -0.86484    -0.90213312 -0.84598282]\n",
      "  [-0.84833127 -0.86196404 -0.89434236 -0.91190347]\n",
      "  [-0.91184954 -0.91483747 -0.91160532 -0.91140238]\n",
      "  [-0.92170294 -0.93275692 -0.9418155  -0.95778067]]]\n",
      "[[-0.93300475]]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "std_scaler = joblib.load('EU_scaler_std.bin')\n",
    "data = pd.read_csv('../data/forecast_EURUSD.csv',index_col=[0]) \n",
    "# initializing DataFormatting class\n",
    "data_init = DataFormatting()\n",
    "df_data, df_datetime = DataFormatting.dataset(data)\n",
    "print('\\n')\n",
    "print('Displaying top 5 rows of the dataset:')\n",
    "print('\\n')\n",
    "print(df_data.head())\n",
    "print(df_data.shape)\n",
    "print(df_data.columns)\n",
    "df_colnames = list(df_data.columns)\n",
    "# normalize train, val and test dataset\n",
    "\n",
    "# initialize StandartScaler()\n",
    "#scaler = StandardScaler()\n",
    "#std_scaler = std_scaler.fit(df_data)\n",
    "data_fit_transformed = std_scaler.transform(df_data)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Displaying top 5 rows of all the scaled dataset:')\n",
    "print('\\n')\n",
    "#print('The train dateset:','\\n''\\n',data_fit_transformed[0:5],'\\n''\\n', 'The validation dataset:','\\n''\\n',val_transformed[0:5],'\\n''\\n','The test dataset:','\\n''\\n',test_transformed[0:5])\n",
    "print('The train dateset:','\\n''\\n',data_fit_transformed)\n",
    "\n",
    "# changing shape of the data to match the model requirement!\n",
    "\n",
    "X_data, y_data = data_transformation(data_fit_transformed, lags = lag, n_fut = n_fut)\n",
    "print('\\n')\n",
    "print('Displaying the shape of the dataset required by the model:')\n",
    "print('\\n')\n",
    "print(f' Input shape X:',X_data.shape, f'Input shape y:',y_data.shape)\n",
    "print('\\n')\n",
    "print(X_data)\n",
    "#print(y_data[0:5])\n",
    "print(y_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 591ms/step\n"
     ]
    }
   ],
   "source": [
    "path_model = \"../Model_Outputs/2022_11_12/EURUSD/model_Bilstm/model/lstm_192.h5\"\n",
    "model_eval = load_model(path_model, compile=False)\n",
    "forecast = model_eval.predict(X_data)\n",
    "forecast_copies = np.repeat(forecast, df_data.shape[1], axis = -1 )\n",
    "y_pred_fut = std_scaler.inverse_transform(forecast_copies)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0015833]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39410d665ad54f46fb296cb711c5a6cfcd8aeec8eb6a5a752176048eef25518b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
